# 03-复杂度分析(上)：如何统计分析算法的执行效率和资源消耗

## 内容摘要

- 复杂度分析是整个算法学习的精髓，只要掌握了它，数据结构和算法的内容基本上就掌握了一半。

### 为什么要复杂度分析

**事后统计方法:** 这种方法主要是通过设计好的测试程序和数据，利用计算机计时器对不同算法编制的程序的运行时间进行比较，从而确定算法效率的高低。

但这种方法显然是有很大缺陷的:

- 必须依据算法事先编制好程序，这通常需要花费大量的时间和精力。
- 时间的比较依赖计算机硬件和软件等环境因素，有时会掩盖算法本身的优劣。
- 算法的测试数据设计困难，并且程序的运行时间往往还与测试数据的规模有很大关系。比如10个数字的排序，不管用什么算沽， 差异几乎是零。而如果有一百万个随机数字排序，那不同算法的差异就非常大了。那么我们为了比较算沽，到底用多少数据来测试，这是很难判断的问题。

**所以，我们需要一个不用具体的测试数据来测试，就可以粗略地估计算法的执行效率的方法。这就是我们今天要讲的时间、空间复杂度分析方法。**

### 大 O 复杂度表示法

算法的执行效率，粗略地讲，就是**算法代码执行的时间**

**估算下面代码的执行时间:**

```c
 int cal(int n) {
   int sum = 0;
   int i = 1;
   for (; i <= n; ++i) {
     sum = sum + i;
   }
   return sum;
 }
```
从 `CPU` 的角度来看，这段代码的每一行都执行着类似的操作：**读数据-运算-写数据**。尽管每行代码对应的 `CPU` 执行的个数、执行的时间都不一样，但是，我们这里只是粗略估计，所以可以假设每行代码执行的时间都一样，为 `unit_time`。在这个假设的基础之上，这段代码的总执行时间是多少呢？

第 2、3 行代码分别需要 1 个 `unit_time` 的执行时间，第 4、5 行都运行了 `n` 遍，所以需要 `2n*unit_time` 的执行时间，所以这段代码总的执行时间就是 `(2n+2)*unit_time`。可以看出来，**所有代码的执行时间 `T(n)` 与每行代码的执行次数成正比**。

```c
 int cal(int n) {
   int sum = 0;
   int i = 1;
   for (; i <= n; ++i) {
     sum = sum + i;
   }
   return sum;
 }
```

第 2、3、4 行代码，每行都需要 1 个 $unit_time$ 的执行时间，第 5、6 行代码循环执行了 $n$ 遍，需要 $2n*unit_time$ 的执行时间，第 7、8 行代码循环执行了 $n^2$遍，所以需要 $2n^2*unit_time$ 的执行时间。所以，整段代码总的执行时间 $T(n) = (2n^2+2n+3)*unit_time$。

**所有代码的执行时间 T(n) 与每行代码的执行次数 n 成正比**。

$$
T(n) = O(f(n))
$$

我来具体解释一下这个公式。其中，$T(n)$ 我们已经讲过了，它表示代码执行的时间；$n$ 表示数据规模的大小；$f(n)$ 表示每行代码执行的次数总和。因为这是一个公式，所以用 $f(n)$ 来表示。公式中的 $O$，表示代码的执行时间 $T(n)$ 与 $f(n)$ 表达式成正比。

所以，第一个例子中的 $T(n) = O(2n+2)$，第二个例子中的 $T(n) = O(2n2+2n+3)$。这就是大 $O$ 时间复杂度表示法。大 $O$ 时间复杂度实际上并不具体表示代码真正的执行时间，而是**表示代码执行时间随数据规模增长的变化趋势**，所以，也叫作**渐进时间复杂度**（asymptotic time complexity），简称**时间复杂度**。

当 $n$ 很大时，你可以把它想象成 10000、100000。而公式中的低阶、常量、系数三部分并不左右增长趋势，所以都可以忽略。我们只需要记录一个最大量级就可以了，如果用大 $O$ 表示法表示刚讲的那两段代码的时间复杂度，就可以记为：$T(n) = O(n)$； $T(n) = O(n^2)$。

### 时间复杂度分析

1. **只关注循环执行次数最多的一段代码**

我刚才说了，大 O 这种复杂度表示方法只是表示一种变化趋势。我们通常会**忽略掉公式中的常量、低阶、系数，只需要记录一个最大阶的量级**就可以了。所以，我们在分析一个算法、一段代码的时间复杂度的时候，也**只关注循环执行次数最多的那一段代码就可以了**。这段核心代码执行次数的 $n$ 的量级，就是整段要分析代码的时间复杂度。

2. **加法法则：总复杂度等于量级最大的那段代码的复杂度**

**总的时间复杂度就等于量级最大的那段代码的时间复杂度**。那我们将这个规律抽象成公式就是：

$$
如果 T1(n)=O(f(n))，T2(n)=O(g(n))；那么 T(n)=T1(n)+T2(n)=max(O(f(n)), O(g(n))) =O(max(f(n), g(n))).
$$

3. **乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积**

$$
如果 T1(n)=O(f(n))，T2(n)=O(g(n))；那么 T(n)=T1(n)*T2(n)=O(f(n))*O(g(n))=O(f(n)*g(n)).
$$

### 几种常见时间复杂度实例分析

![复杂度量级](_v_images/20200525215059458_22527.jpg)

对于刚罗列的复杂度量级，我们可以粗略地分为两类，多项式量级和非多项式量级。其中，非多项式量级只有两个：$O(2n) 和 O(n!)$。我们把时间复杂度为非多项式量级的算法问题叫作 NP（Non-Deterministic Polynomial，非确定多项式）问题。当数据规模 $n$ 越来越大时，非多项式量级算法的执行时间会急剧增加，求解问题的执行时间会无限增长。所以，非多项式时间复杂度的算法其实是非常低效的算法。因此，关于 NP 时间复杂度我就不展开讲了。我们主要来看几种常见的多项式时间复杂度。

1. $O(1)$

**一般情况下，只要算法中不存在循环语句、递归语句，即使有成千上万行的代码，其时间复杂度也是Ο(1)。**

2. $O(logn)、O(nlogn)$

```c
 i=1;
 while (i <= n)  {
   i = i * 2;
 }
```

从代码中可以看出，变量 i 的值从 1 开始取，每循环一次就乘以 2。当大于 n 时，循环结束。还记得我们高中学过的等比数列吗？实际上，变量 i 的取值就是一个等比数列。如果我把它一个一个列出来，就应该是这个样子的:

$$
2^0 2^1 2^2 2^3 ……2^x = n
$$

所以，我们只要知道 x 值是多少，就知道这行代码执行的次数了。通过 2x=n 求解 x 这个问题我们想高中应该就学过了，我就不多说了。x=log2n，所以，这段代码的时间复杂度就是 O(log2n)。

```c
// O(log3n)
 i=1;
 while (i <= n)  {
   i = i * 3;
 }
```

- **在采用大 O 标记复杂度的时候，可以忽略系数，即 O(Cf(n)) = O(f(n))**

因此，在对数阶时间复杂度的表示方法里，我们忽略对数的“底”，统一表示为 O(logn)。如果一段代码的时间复杂度是 O(logn)，我们循环执行 n 遍，时间复杂度就是 O(nlogn) 了。而且，O(nlogn) 也是一种非常常见的算法时间复杂度。比如，**归并排序、快速排序**的时间复杂度都是 O(nlogn)。

3. $O(m+n)、O(m*n)$
 
我们再来讲一种跟前面都不一样的时间复杂度，**代码的复杂度由两个数据的规模来决定**。老规矩，先看代码！

```c
int cal(int m, int n) {
  int sum_1 = 0;
  int i = 1;
  for (; i < m; ++i) {
    sum_1 = sum_1 + i;
  }

  int sum_2 = 0;
  int j = 1;
  for (; j < n; ++j) {
    sum_2 = sum_2 + j;
  }

  return sum_1 + sum_2;
}
```

从代码中可以看出，m 和 n 是表示两个数据规模。我们无法事先评估 m 和 n 谁的量级大，所以我们在表示复杂度的时候，就不能简单地利用加法法则，省略掉其中一个。所以，上面代码的时间复杂度就是 $O(m+n)$。

针对这种情况，原来的加法法则就不正确了，我们需要将加法规则改为：$T1(m) + T2(n) = O(f(m) + g(n))$。但是乘法法则继续有效：$T1(m)*T2(n) = O(f(m) * f(n))$。

### 空间复杂度分析

- **时间复杂度的全称是渐进时间复杂度，表示算法的执行时间与数据规模之间的增长关系**。
- 类比一下，**空间复杂度全称就是渐进空间复杂度（asymptotic space complexity），表示算法的存储空间与数据规模之间的增长关系**。

```c
void print(int n) {
  int i = 0;
  int[] a = new int[n];
  for (i; i <n; ++i) {
    a[i] = i * i;
  }

  for (i = n-1; i >= 0; --i) {
    print out a[i]
  }
}
```

跟时间复杂度分析一样，我们可以看到，第 2 行代码中，我们申请了一个空间存储变量 i，但是它是常量阶的，跟数据规模 n 没有关系，所以我们可以忽略。第 3 行申请了一个大小为 n 的 `int` 类型数组，除此之外，剩下的代码都没有占用更多的空间，所以整段代码的空间复杂度就是 $O(n)$。

我们常见的空间复杂度就是 $O(1)、O(n)、O(n^2)，像 O(logn)、O(nlogn)$ 这样的对数阶复杂度平时都用不到。而且，空间复杂度分析比时间复杂度分析要简单很多。所以，对于空间复杂度，掌握刚我说的这些内容已经足够了。

## 总结与思考

复杂度也叫**渐进复杂度，包括时间复杂度和空间复杂度，用来分析算法执行效率与数据规模之间的增长关系**，可以粗略地表示，越高阶复杂度的算法，执行效率越低。常见的复杂度并不多，从低阶到高阶有：$O(1)、O(logn)、O(n)、O(nlogn)、O(n2)$。

![复杂度曲线](_v_images/20200526073909846_12564.jpg)

## 评论

总结
一、什么是复杂度分析？
1.数据结构和算法解决是“如何让计算机更快时间、更省空间的解决问题”。
2.因此需从执行时间和占用空间两个维度来评估数据结构和算法的性能。
3.分别用时间复杂度和空间复杂度两个概念来描述性能问题，二者统称为复杂度。
4.复杂度描述的是算法执行时间（或占用空间）与数据规模的增长关系。
二、为什么要进行复杂度分析？
1.和性能测试相比，复杂度分析有不依赖执行环境、成本低、效率高、易操作、指导性强的特点。
2.掌握复杂度分析，将能编写出性能更优的代码，有利于降低系统开发和维护成本。
三、如何进行复杂度分析？
1.大O表示法
1）来源
算法的执行时间与每行代码的执行次数成正比，用T(n) = O(f(n))表示，其中T(n)表示算法执行总时间，f(n)表示每行代码执行总次数，而n往往表示数据的规模。
2）特点
以时间复杂度为例，由于时间复杂度描述的是算法执行时间与数据规模的增长变化趋势，所以常量阶、低阶以及系数实际上对这种增长趋势不产决定性影响，所以在做时间复杂度分析时忽略这些项。
2.复杂度分析法则
1）单段代码看高频：比如循环。
2）多段代码取最大：比如一段代码中有单循环和多重循环，那么取多重循环的复杂度。
3）嵌套代码求乘积：比如递归、多重循环等
4）多个规模求加法：比如方法有两个参数控制两个循环的次数，那么这时就取二者复杂度相加。
四、常用的复杂度级别？
多项式阶：随着数据规模的增长，算法的执行时间和空间占用，按照多项式的比例增长。包括，
O(1)（常数阶）、O(logn)（对数阶）、O(n)（线性阶）、O(nlogn)（线性对数阶）、O(n^2)（平方阶）、O(n^3)（立方阶）
非多项式阶：随着数据规模的增长，算法的执行时间和空间占用暴增，这类算法性能极差。包括，
O(2^n)（指数阶）、O(n!)（阶乘阶）
五、如何掌握好复杂度分析方法？
复杂度分析关键在于多练，所谓孰能生巧。

-------

文章里也说了，性能测试这种是受环境所影响的。作为程序员，我们能做的就是尽可能的降低复杂度，才能让代码在不同的环境下以最快的效率执行。至于是不是浪费时间，我觉得其实是个伪命题。首先按刚刚分析过程来看，通过熟悉练习，简单的代码是可以直接看出来复杂度的也就是不费时间；而比较复杂的代码就容易“一不小心”太“复杂”了，这个时候，为了代码质量考虑分析复杂度的时间也并不浪费。再有甚者，**我们学习这个分析法，我觉得更多的是要明白这个理念，在写代码的时候就能关注一下这方面的问题，毕竟复杂的代码在写的过程往往是先分析整体逻辑结构的**，并且写的过程也需要不断思考，了解这个理念后才能在写的过程中也思考关注这个点。不然，复杂的一段代码一旦写成，日后因为性能问题重构，更费时间。

------

**思考题，性能测试与复杂度分析不冲突**，原因如下：
1、性能测试是依附于具体的环境，如SIT、UAT机器配置及实例数量不一致结果也有差别。
2、复杂度分析是独立于环境的，可以大致估算出程序所执行的效率。
3、将复杂度熟记于心，能够写出更高效率、更好性能的代码。若某接口通过性能测试，达不到预期，还可以用复杂度分析接口代码，找出最影响性能的代码，进行优化。

每段代码都分析一下时间复杂度、空间复杂度，是不是很浪费时间呢？
这个问题分两种情况讨论
1、开发过程中，码代码的过程中就能得出其复杂度，这并不会太多的浪费时间，同时只有分析了每段代码的复杂度，才能估算出它们的执行效率。
2、优化代码时，只有在分析每段代码的复杂度后，才能定位问题代码，才能做相应优化
